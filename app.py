import streamlit as st
import requests
import pandas as pd
from rapidfuzz import fuzz, process
import io

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ç–æ—Ä –≥–µ–æ HH.ru",
    page_icon="üåç",
    layout="wide"
)

# CSS –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ –∑–µ–º–ª–∏ –∏ —Å—Ç–∏–ª–µ–π –∑–∞–≥–æ–ª–æ–≤–∫–∞
st.markdown("""
<style>
@keyframes rotate {
    from { transform: rotate(0deg); }
    to { transform: rotate(360deg); }
}

.rotating-earth {
    display: inline-block;
    animation: rotate 3s linear infinite;
    font-size: 3em;
    vertical-align: middle;
    margin-right: 15px;
}

.main-title {
    display: inline-block;
    font-size: 3em;
    font-weight: bold;
    vertical-align: middle;
    margin: 0;
}

.title-container {
    display: flex;
    align-items: center;
    margin-bottom: 20px;
}
</style>
""", unsafe_allow_html=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state
if 'result_df' not in st.session_state:
    st.session_state.result_df = None
if 'processed' not in st.session_state:
    st.session_state.processed = False
if 'manual_selections' not in st.session_state:
    st.session_state.manual_selections = {}
if 'candidates_cache' not in st.session_state:
    st.session_state.candidates_cache = {}

# ============================================
# –§–£–ù–ö–¶–ò–ò
# ============================================
@st.cache_data(ttl=3600)
def get_hh_areas():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ HH.ru"""
    response = requests.get('https://api.hh.ru/areas')
    data = response.json()
    
    areas_dict = {}
    
    def parse_areas(areas, parent_name=""):
        for area in areas:
            area_id = area['id']
            area_name = area['name']
            
            areas_dict[area_name] = {
                'id': area_id,
                'name': area_name,
                'parent': parent_name
            }
            
            if area.get('areas'):
                parse_areas(area['areas'], area_name)
    
    parse_areas(data)
    return areas_dict

def normalize_region_name(text):
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    text = text.lower()
    replacements = {
        '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è': '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥', '–º–æ—Å–∫–æ–≤—Å–∫–∞—è': '–º–æ—Å–∫–æ–≤', '–∫—É—Ä—Å–∫–∞—è': '–∫—É—Ä—Å–∫',
        '–∫–µ–º–µ—Ä–æ–≤—Å–∫–∞—è': '–∫–µ–º–µ—Ä–æ–≤', '—Å–≤–µ—Ä–¥–ª–æ–≤—Å–∫–∞—è': '—Å–≤–µ—Ä–¥–ª–æ–≤', '–Ω–∏–∂–µ–≥–æ—Ä–æ–¥—Å–∫–∞—è': '–Ω–∏–∂–µ–≥–æ—Ä–æ–¥',
        '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è': '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫', '—Ç–∞–º–±–æ–≤—Å–∫–∞—è': '—Ç–∞–º–±–æ–≤', '–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫–∞—è': '–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫',
        '–æ–±–ª–∞—Å—Ç—å': '', '–æ–±–ª': '', '–∫—Ä–∞–π': '', '—Ä–µ—Å–ø—É–±–ª–∏–∫–∞': '', '—Ä–µ—Å–ø': '', '  ': ' '
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text.strip()

def extract_city_and_region(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –∏ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_lower = text.lower()
    region_keywords = [
        '–æ–±–ª–∞—Å—Ç', '–∫—Ä–∞–π', '—Ä–µ—Å–ø—É–±–ª–∏–∫', '–æ–∫—Ä—É–≥', '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥', '–º–æ—Å–∫–æ–≤', '–∫—É—Ä—Å–∫', '–∫–µ–º–µ—Ä–æ–≤',
        '—Å–≤–µ—Ä–¥–ª–æ–≤', '–Ω–∏–∂–µ–≥–æ—Ä–æ–¥', '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫', '—Ç–∞–º–±–æ–≤', '–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫'
    ]
    words = text.split()
    if len(words) == 1:
        return text, None
    city_words, region_words = [], []
    region_found = False
    for word in words:
        if not region_found and any(keyword in word.lower() for keyword in region_keywords):
            region_found = True
        if region_found:
            region_words.append(word)
        else:
            city_words.append(word)
    city = ' '.join(city_words) if city_words else text
    region = ' '.join(region_words) if region_words else None
    return city, region

def check_if_changed(original, matched):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–æ—Å—å –ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"""
    if matched is None or matched == "‚ùå –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è":
        return False
    return original.strip() != matched.strip()

def get_candidates_by_word(client_city, hh_city_names, limit=20):
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞"""
    first_word = client_city.split()[0].lower().strip()
    candidates = []
    for city_name in hh_city_names:
        if first_word in city_name.lower():
            score = fuzz.WRatio(client_city.lower(), city_name.lower())
            candidates.append((city_name, score))
    candidates.sort(key=lambda x: x[1], reverse=True)
    return candidates[:limit]

def smart_match_city(client_city, hh_city_names, hh_areas, threshold=85):
    """–£–º–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
    match_result, candidates = None, []
    word_candidates = get_candidates_by_word(client_city, hh_city_names)
    if word_candidates and word_candidates[0][1] >= threshold:
        match_result = (word_candidates[0][0], word_candidates[0][1], 0)
    return match_result, word_candidates

def match_cities(client_cities, hh_areas, threshold=85):
    """–°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≥–æ—Ä–æ–¥–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
    results = []
    hh_city_names = list(hh_areas.keys())
    seen_original_cities, seen_hh_cities = {}, {}
    duplicate_original_count, duplicate_hh_count = 0, 0
    st.session_state.candidates_cache = {}
    progress_bar = st.progress(0)
    status_text = st.empty()

    for idx, client_city in enumerate(client_cities):
        progress_bar.progress((idx + 1) / len(client_cities))
        status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx + 1} –∏–∑ {len(client_cities)} –≥–æ—Ä–æ–¥–æ–≤...")
        
        row_data = {'–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city, 'row_id': idx}
        if pd.isna(client_city) or str(client_city).strip() == "":
            results.append({**row_data, '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': None, 'ID HH': None, '–†–µ–≥–∏–æ–Ω': None, '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': 0, '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–ù–µ—Ç', '–°—Ç–∞—Ç—É—Å': '‚ùå –ü—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ'})
            continue

        client_city_original = str(client_city).strip()
        client_city_normalized = client_city_original.lower().strip()

        if client_city_normalized in seen_original_cities:
            duplicate_original_count += 1
            original_result = seen_original_cities[client_city_normalized]
            results.append({**row_data, **original_result, '–°—Ç–∞—Ç—É—Å': 'üîÑ –î—É–±–ª–∏–∫–∞—Ç (–∏—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)'})
            continue

        match_result, candidates = smart_match_city(client_city_original, hh_city_names, hh_areas, threshold)
        st.session_state.candidates_cache[idx] = candidates

        if match_result:
            matched_name, score = match_result[0], match_result[1]
            hh_info = hh_areas[matched_name]
            hh_city_normalized = hh_info['name'].lower().strip()
            is_changed = check_if_changed(client_city_original, hh_info['name'])
            
            city_result = {
                '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': hh_info['name'], 'ID HH': hh_info['id'], '–†–µ–≥–∏–æ–Ω': hh_info['parent'],
                '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': round(score, 1), '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–î–∞' if is_changed else '–ù–µ—Ç'
            }

            if hh_city_normalized in seen_hh_cities:
                duplicate_hh_count += 1
                results.append({**row_data, **city_result, '–°—Ç–∞—Ç—É—Å': 'üîÑ –î—É–±–ª–∏–∫–∞—Ç (—Ä–µ–∑—É–ª—å—Ç–∞—Ç HH)'})
            else:
                status = '‚úÖ –¢–æ—á–Ω–æ–µ' if score >= 95 else '‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ–µ'
                results.append({**row_data, **city_result, '–°—Ç–∞—Ç—É—Å': status})
                seen_hh_cities[hh_city_normalized] = True
            
            seen_original_cities[client_city_normalized] = city_result
        else:
            results.append({**row_data, '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': None, 'ID HH': None, '–†–µ–≥–∏–æ–Ω': None, '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': 0, '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–ù–µ—Ç', '–°—Ç–∞—Ç—É—Å': '‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ'})

    progress_bar.empty()
    status_text.empty()
    return pd.DataFrame(results), duplicate_original_count, duplicate_hh_count, duplicate_original_count + duplicate_hh_count

# ============================================
# –ò–ù–¢–ï–†–§–ï–ô–°
# ============================================
st.markdown(
    '<div class="title-container"><span class="rotating-earth">üåç</span>'
    '<span class="main-title">–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ç–æ—Ä –≥–µ–æ HH.ru</span></div>',
    unsafe_allow_html=True
)
st.markdown("---")

# ----- SIDEBAR -----
with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    threshold = st.slider("–ü–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (%)", min_value=50, max_value=100, value=85, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è.")
    st.markdown("---")
    st.markdown("### üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è")
    st.markdown("1. **–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª** Excel –∏–ª–∏ CSV —Å –≥–æ—Ä–æ–¥–∞–º–∏.\n"
                "2. –ì–æ—Ä–æ–¥–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ **–ø–µ—Ä–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ**.\n"
                "3. –ù–∞–∂–º–∏—Ç–µ **'üöÄ –ù–∞—á–∞—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ'**.\n"
                "4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.\n"
                "5. –°–∫–∞—á–∞–π—Ç–µ –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª.")
    st.markdown("---")
    st.markdown("### üìä –°—Ç–∞—Ç—É—Å—ã")
    st.markdown("- ‚úÖ **–¢–æ—á–Ω–æ–µ**: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ ‚â•95%\n"
                "- ‚ö†Ô∏è **–ü–æ—Ö–æ–∂–µ–µ**: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ ‚â•–ø–æ—Ä–æ–≥–∞\n"
                "- üîÑ **–î—É–±–ª–∏–∫–∞—Ç**: –ø–æ–≤—Ç–æ—Ä—ã\n"
                "- ‚ùå **–ù–µ –Ω–∞–π–¥–µ–Ω–æ**: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ <–ø–æ—Ä–æ–≥–∞")
    with st.expander("üìö –ü–æ–ª–Ω–∞—è —Å–ø—Ä–∞–≤–∫–∞", expanded=False):
        st.markdown("–°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–∞—à —Å–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–º HeadHunter. –î–ª—è –≥–æ—Ä–æ–¥–æ–≤ —Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º ‚â§ 90% –¥–æ—Å—Ç—É–ø–Ω–æ —Ä—É—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–∏—Å–∫ –Ω–∞–¥ —Ç–∞–±–ª–∏—Ü–µ–π (–≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏ –Ω–∞–∂–º–∏—Ç–µ Enter) –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")

# ----- –û–°–ù–û–í–ù–ê–Ø –ß–ê–°–¢–¨ -----
col1, col2 = st.columns([1, 1])
with col1:
    st.subheader("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞")
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª —Å –≥–æ—Ä–æ–¥–∞–º–∏", type=['xlsx', 'csv'], help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã: Excel (.xlsx) –∏ CSV.")
    with st.expander("üìã –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞"):
        st.dataframe(pd.DataFrame({'–ù–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤': ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', '–ö–∏—Ä–æ–≤—Å–∫ –õ–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å']}), use_container_width=True, hide_index=True)
with col2:
    st.subheader("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
    try:
        hh_areas = get_hh_areas()
        st.success(f"‚úÖ –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ HH –∑–∞–≥—Ä—É–∂–µ–Ω: **{len(hh_areas)}** –≥–µ–æ-–æ–±—ä–µ–∫—Ç–æ–≤.")
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ HH: {str(e)}")
        hh_areas = None

if uploaded_file is not None and hh_areas is not None:
    st.markdown("---")
    df = pd.read_excel(uploaded_file) if uploaded_file.name.endswith('.xlsx') else pd.read_csv(uploaded_file)
    client_cities = df.iloc[:, 0].dropna().unique().tolist()
    st.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ **{len(df)}** —Å—Ç—Ä–æ–∫ (**{len(client_cities)}** —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤) –∏–∑ —Ñ–∞–π–ª–∞.")
    
    if st.button("üöÄ –ù–∞—á–∞—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ", type="primary", use_container_width=True):
        with st.spinner("–ò–¥–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è."):
            result_df, dup_orig, dup_hh, total_dup = match_cities(client_cities, hh_areas, threshold)
            st.session_state.result_df = result_df
            st.session_state.dup_original = dup_orig
            st.session_state.dup_hh = dup_hh
            st.session_state.processed = True
            st.session_state.manual_selections = {}

    if st.session_state.processed and st.session_state.result_df is not None:
        result_df = st.session_state.result_df.copy()
        
        st.markdown("---")
        st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        total = len(result_df)
        to_export = len(result_df[~result_df['–°—Ç–∞—Ç—É—Å'].str.contains('–î—É–±–ª–∏–∫–∞—Ç', na=False) & result_df['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ'].notna()])
        
        cols = st.columns(6)
        cols[0].metric("–í—Å–µ–≥–æ", total)
        cols[1].metric("‚úÖ –¢–æ—á–Ω—ã—Ö", len(result_df[result_df['–°—Ç–∞—Ç—É—Å'] == '‚úÖ –¢–æ—á–Ω–æ–µ']))
        cols[2].metric("‚ö†Ô∏è –ü–æ—Ö–æ–∂–∏—Ö", len(result_df[result_df['–°—Ç–∞—Ç—É—Å'] == '‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ–µ']))
        cols[3].metric("üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤", len(result_df[result_df['–°—Ç–∞—Ç—É—Å'].str.contains('–î—É–±–ª–∏–∫–∞—Ç', na=False)]))
        cols[4].metric("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ", len(result_df[result_df['–°—Ç–∞—Ç—É—Å'] == '‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ']))
        cols[5].metric("üì§ –ö –≤—ã–≥—Ä—É–∑–∫–µ", to_export)

        st.markdown("---")
        st.subheader("üìã –¢–∞–±–ª–∏—Ü–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π")

        # –ü—Ä–æ—Å—Ç–æ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞ (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ Enter)
        search_query = st.text_input(
            "üîç –ü–æ–∏—Å–∫ –ø–æ —Ç–∞–±–ª–∏—Ü–µ (–Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è)",
            placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏ –Ω–∞–∂–º–∏—Ç–µ Enter..."
        )
        
        result_df_sorted = result_df.sort_values(by='–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %').reset_index(drop=True)
        
        # –õ–æ–≥–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        if search_query:
            search_lower = search_query.lower()
            mask = result_df_sorted.apply(lambda row: any(search_lower in str(val).lower() for val in row), axis=1)
            result_df_filtered = result_df_sorted[mask]
        else:
            result_df_filtered = result_df_sorted
        
        st.dataframe(result_df_filtered.drop(['row_id'], axis=1, errors='ignore'), use_container_width=True, height=400)
        
        editable_rows = result_df[result_df['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %'] <= 90].copy()
        if not editable_rows.empty:
            st.markdown("---")
            st.subheader("‚úèÔ∏è –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–æ–≤ (—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ ‚â§ 90%)")
            for _, row in editable_rows.iterrows():
                row_id = row['row_id']
                default_val = st.session_state.manual_selections.get(row_id, row['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ'])
                candidates = st.session_state.candidates_cache.get(row_id, [])
                options = ["‚ùå –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"] + [c[0] for c in candidates]
                
                try:
                    default_idx = options.index(default_val) if default_val in options else 0
                except ValueError:
                    default_idx = 0
                
                selected = st.selectbox(f"**{row['–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ']}** (–Ω–∞–π–¥–µ–Ω–æ: *{row['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ']}* | {row['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %']}%)", options, index=default_idx, key=f"select_{row_id}")
                st.session_state.manual_selections[row_id] = selected

        st.markdown("---")
        st.subheader("üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        cols_dl = st.columns(3)
        final_result_df = result_df.copy()
        for row_id, new_value in st.session_state.manual_selections.items():
            mask = final_result_df['row_id'] == row_id
            if new_value == "‚ùå –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è":
                final_result_df.loc[mask, ['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ', 'ID HH', '–†–µ–≥–∏–æ–Ω', '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %', '–°—Ç–∞—Ç—É—Å']] = [None, None, None, 0, '‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ']
            else:
                final_result_df.loc[mask, '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ'] = new_value
                if new_value in hh_areas:
                    final_result_df.loc[mask, 'ID HH'] = hh_areas[new_value]['id']
                    final_result_df.loc[mask, '–†–µ–≥–∏–æ–Ω'] = hh_areas[new_value]['parent']
        
        def to_excel(df: pd.DataFrame):
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False, header=False, sheet_name='–ì–µ–æ')
            return output.getvalue()

        if st.session_state.manual_selections:
            publisher_manual_df = final_result_df[~final_result_df['–°—Ç–∞—Ç—É—Å'].str.contains('–î—É–±–ª–∏–∫–∞—Ç', na=False) & final_result_df['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ'].notna()][['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ']]
            manual_count = len(publisher_manual_df)
            percentage = (manual_count / total * 100) if total > 0 else 0
            cols_dl[0].download_button(
                label=f"‚úèÔ∏è –° —Ä—É—á–Ω—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ ({manual_count} | {percentage:.0f}%)",
                data=to_excel(publisher_manual_df),
                file_name=f"geo_manual_{uploaded_file.name.split('.')[0]}.xlsx",
                mime="application/vnd.ms-excel",
                use_container_width=True, type="primary"
            )
        
        full_report_df = final_result_df.drop(['row_id'], axis=1, errors='ignore')
        cols_dl[1].download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç",
            data=full_report_df.to_csv(index=False).encode('utf-8'),
            file_name=f"result_{uploaded_file.name.split('.')[0]}.csv",
            mime="text/csv",
            use_container_width=True
        )

        publisher_df = final_result_df[~final_result_df['–°—Ç–∞—Ç—É—Å'].str.contains('–î—É–±–ª–∏–∫–∞—Ç', na=False) & final_result_df['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ'].notna()][['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ']]
        cols_dl[2].download_button(
            label=f"üì§ –§–∞–π–ª –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ç–æ—Ä–∞ ({len(publisher_df)})",
            data=to_excel(publisher_df),
            file_name=f"geo_for_publisher_{uploaded_file.name.split('.')[0]}.xlsx",
            mime="application/vnd.ms-excel",
            use_container_width=True
        )

st.markdown("---")
st.markdown("–°–¥–µ–ª–∞–Ω–æ —Å ‚ù§Ô∏è | –î–∞–Ω–Ω—ã–µ –∏–∑ API HH.ru")
